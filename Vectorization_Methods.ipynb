{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e381816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709688a8",
   "metadata": {},
   "source": [
    "The newsgroup dataset is a collection of 20,000 newsgroup documents positioned evenly across 20 different newsgroups. A newsgroup is a forum on the Usenet service for the discussion of a particular topic. Usenet is a distributed discussion system popular in the 80s and early 90s which allowed people to post articles to various newsgroups. Each document in the dataset is a single post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf4428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "data = newsgroups.data\n",
    "target = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f84d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing set. \n",
    "# The test set will be 20% of the dataset, following the 80-20 relation.\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae502c9",
   "metadata": {},
   "source": [
    "## `CountVectorizer` ##\n",
    "`CountVectorizer` is scikit-learn's implementation of a generic vectorization method. Different parameters can be added to tweak aspects of the process. The general steps used are:\n",
    "1. Tokenization\n",
    "2. Lowercasing\n",
    "3. Removing Stop Words\n",
    "4. Building a Vocabulary (Finding all unique words in the corpus). This step determines n--the length of the resulting vectors.\n",
    "5. Counting occurrences\n",
    "6. Output one vector for each document in the corpus\n",
    "\n",
    "`CountVectorizer` also allows an N-gram range to be specified, allowing combinations of words to be captured in the resulting vector. Using N-gram will increase the vector space and potentially make it prohibitively large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fa3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words model\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_train = bow_vectorizer.fit_transform(data_train)\n",
    "bow_test = bow_vectorizer.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902754fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams model, this model captures only combinations of two tokens (no single words)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "ngram_train = ngram_vectorizer.fit_transform(data_train)\n",
    "ngram_test = ngram_vectorizer.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383c568",
   "metadata": {},
   "source": [
    "## `TfidfTransformer` ##\n",
    "\n",
    "TfidfTransformer will compute a normalized Term Frequency vector, which divides the counts by the total number of words in the document. If the flag `use_idf=True`, then Inverse Document Frequency will also be calculated. Inverse Document Frequency is the logarithmically scaled inverse fraction of the documents that contain the word. IDF is used to measure the importance of a word relative to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a69476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words model can be updated to account for term frequency (TF) in scikit-learn\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(bow_train)\n",
    "bow_train_tf = tf_transformer.transform(bow_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951e5db",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "\n",
    "A Multinomial Naive Bayes Classifier is a Naive Bayes classifier (Bayes Theorem with the assumption that all features are independent from each other) which assumes a multinomial distribution for the data. This is a good assumption to make for text data, where the features are counts of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc19ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.sys.ibm.pc.hardware\n"
     ]
    }
   ],
   "source": [
    "# Train an example classifier (Multinomial Naive Bayes) to predict the category of a post\n",
    "classifier = MultinomialNB().fit(bow_train_tf, target_train)\n",
    "\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new = bow_vectorizer.transform(docs_new) \n",
    "X_new_tf = tf_transformer.transform(X_new)\n",
    "\n",
    "predicted = classifier.predict(X_new_tf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, newsgroups.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15daa79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline, which allows one to apply multiple transformations together.\n",
    "text_classifier = Pipeline([\n",
    "    ('tf', TfidfTransformer(use_idf=True)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b5349",
   "metadata": {},
   "source": [
    "## Performance Evaluation ##\n",
    "Now we can evaluate the performance of difference forms of vectorization and compare them to one another on a couple of classifiers. With a MultinomialNB classifier, we achieve slightly better results by using bigram vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df539a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved for Bag-of-Words model is  0.7652519893899205\n",
      "Accuracy achieved for 2-grams model is  0.8010610079575596\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance\n",
    "text_classifier.fit(bow_train, target_train)\n",
    "predicted = text_classifier.predict(bow_test)\n",
    "print('Accuracy achieved for Bag-of-Words model is ', np.mean(predicted == target_test))\n",
    "\n",
    "text_classifier.fit(ngram_train, target_train)\n",
    "predicted = text_classifier.predict(ngram_test)\n",
    "print('Accuracy achieved for 2-grams model is ', np.mean(predicted == target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5a704",
   "metadata": {},
   "source": [
    "## Support Vector Machine ##\n",
    "\n",
    "SVM finds the best boundary to separate data points into classes. It is a good classifier for text but has high computational requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830ba31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the classifier used in our Pipeline to a linear Support Vector Machine\n",
    "text_classifier = Pipeline([\n",
    "    ('tf', TfidfTransformer(use_idf=False)),\n",
    "    ('classifier', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                              alpha=1e-3, random_state=42,\n",
    "                              max_iter=5, tol=None))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989f4c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved for Bag-of-Words model is  0.8236074270557029\n",
      "Accuracy achieved for 2-grams model is  0.8697612732095491\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance with the new classifier\n",
    "text_classifier.fit(bow_train, target_train)\n",
    "predicted = text_classifier.predict(bow_test)\n",
    "print('Accuracy achieved for Bag-of-Words model is ', np.mean(predicted == target_test))\n",
    "\n",
    "text_classifier.fit(ngram_train, target_train)\n",
    "predicted = text_classifier.predict(ngram_test)\n",
    "print('Accuracy achieved for 2-grams model is ', np.mean(predicted == target_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
